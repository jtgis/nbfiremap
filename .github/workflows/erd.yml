name: Hourly Fire Data Sync

on:
  schedule:
    - cron: "21 * * * *"  # :21 past every hour (UTC)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install requests pdfplumber

      - name: Fetch ERD fire data
        shell: python
        run: |
          """Fetch NB Emergency Response Division (ERD) fire data."""
          import os
          import io
          import re
          import json
          import time
          import requests

          # =============================================================================
          # Configuration
          # =============================================================================

          OUTPUT_DIR = "erd"
          TIMEOUT = 90
          RETRIES = 3

          GEOJSON_LAYERS = {
              "active_fires": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query",
              "out_fires": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query",
              "fire_locations": "https://gis-erd-der.gnb.ca/gisserver/rest/services/New_Brunswick_Fires/New_Brunswick_Fire_Locations/FeatureServer/0/query",
          }

          TABLE_LAYER = {
              "name": "sums_table",
              "url": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query",
          }

          PDF_CONFIG = {
              "url": "https://www3.gnb.ca/public/fire-feu/activitysum_e.pdf",
              "output": "GNBfireActSum.json",
          }

          # =============================================================================
          # Helpers
          # =============================================================================

          def log(msg: str) -> None:
              ts = time.strftime("%H:%M:%S", time.gmtime())
              print(f"[{ts}] {msg}", flush=True)

          def fetch_json(url: str, params: dict, label: str) -> dict:
              """Fetch JSON from URL with retries."""
              for attempt in range(1, RETRIES + 1):
                  try:
                      log(f"  {label}: GET (attempt {attempt})")
                      r = requests.get(url, params=params, timeout=TIMEOUT)
                      r.raise_for_status()
                      return r.json()
                  except Exception as e:
                      log(f"  {label}: Error - {e}")
                      time.sleep(1 * attempt)
              return {}

          def fetch_arcgis_geojson(url: str, label: str) -> dict:
              """Fetch GeoJSON from ArcGIS REST endpoint."""
              params = {
                  "where": "1=1",
                  "outFields": "*",
                  "outSR": "4326",
                  "returnGeometry": "true",
                  "f": "geojson",
              }
              data = fetch_json(url, params, label)
              
              # Check for ArcGIS error
              if data.get("error"):
                  log(f"  {label}: ArcGIS error - {data['error']}")
                  return {"type": "FeatureCollection", "features": []}
              
              if data.get("type") != "FeatureCollection":
                  log(f"  {label}: Invalid GeoJSON response")
                  return {"type": "FeatureCollection", "features": []}
              
              return data

          def fetch_arcgis_table(url: str, label: str) -> dict:
              """Fetch table data from ArcGIS REST endpoint."""
              params = {
                  "where": "1=1",
                  "outFields": "*",
                  "returnGeometry": "false",
                  "f": "json",
              }
              data = fetch_json(url, params, label)
              
              if data.get("error"):
                  log(f"  {label}: ArcGIS error - {data['error']}")
                  return {"features": []}
              
              return data

          def add_timestamp(data: dict, field: str = "FETCHED_FROM_ERD") -> dict:
              """Add timestamp to features."""
              ts = int(time.time())
              for f in data.get("features", []):
                  if "properties" in f:
                      f["properties"][field] = ts
                  elif "attributes" in f:
                      f["attributes"][field] = ts
              return data

          def write_json(path: str, data: dict) -> None:
              """Write JSON to file."""
              with open(path, "w", encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False, indent=2)
                  f.write("\n")
              log(f"  Wrote {path}")

          # =============================================================================
          # PDF Parsing
          # =============================================================================

          def clean_text(s) -> str:
              if s is None:
                  return ""
              return re.sub(r"\s+", " ", str(s)).strip()

          def parse_pdf(url: str) -> dict:
              """Parse GNB fire activity summary PDF."""
              import pdfplumber
              
              result = {
                  "source": url,
                  "fetched_utc": int(time.time()),
                  "pages": 0,
                  "tables": [],
              }
              
              try:
                  cache_url = f"{url}?t={int(time.time())}"
                  log(f"  Fetching PDF...")
                  r = requests.get(cache_url, timeout=TIMEOUT)
                  r.raise_for_status()
                  
                  with pdfplumber.open(io.BytesIO(r.content)) as pdf:
                      result["pages"] = len(pdf.pages)
                      
                      for page_num, page in enumerate(pdf.pages, 1):
                          tables = page.extract_tables() or []
                          
                          for table in tables:
                              if not table or len(table) < 2:
                                  continue
                              
                              # Parse header
                              header = [clean_text(c) for c in table[0]]
                              keep_idx = [i for i, h in enumerate(header) if h]
                              header = [header[i] for i in keep_idx]
                              
                              if not header:
                                  continue
                              
                              # Parse rows
                              rows = []
                              for row in table[1:]:
                                  if not row:
                                      continue
                                  cleaned = [clean_text(c) for c in row]
                                  values = [cleaned[i] if i < len(cleaned) else "" for i in keep_idx]
                                  
                                  # Pad/trim to header length
                                  while len(values) < len(header):
                                      values.append("")
                                  values = values[:len(header)]
                                  
                                  if any(v for v in values):
                                      rows.append(dict(zip(header, values)))
                              
                              if rows:
                                  result["tables"].append({
                                      "page": page_num,
                                      "headers": header,
                                      "rows": rows,
                                  })
                  
                  log(f"  Parsed {len(result['tables'])} tables")
                  
              except Exception as e:
                  log(f"  PDF error: {e}")
                  result["error"] = str(e)
              
              return result

          # =============================================================================
          # Main
          # =============================================================================

          os.makedirs(OUTPUT_DIR, exist_ok=True)

          # Fetch GeoJSON layers
          for name, url in GEOJSON_LAYERS.items():
              log(f"Fetching {name}...")
              data = fetch_arcgis_geojson(url, name)
              data = add_timestamp(data)
              write_json(os.path.join(OUTPUT_DIR, f"{name}.geojson"), data)
              log(f"  Features: {len(data.get('features', []))}")

          # Fetch table layer
          log(f"Fetching {TABLE_LAYER['name']}...")
          data = fetch_arcgis_table(TABLE_LAYER["url"], TABLE_LAYER["name"])
          data = add_timestamp(data)
          write_json(os.path.join(OUTPUT_DIR, f"{TABLE_LAYER['name']}.json"), data)
          log(f"  Rows: {len(data.get('features', []))}")

          # Parse PDF
          log("Parsing activity summary PDF...")
          pdf_data = parse_pdf(PDF_CONFIG["url"])
          write_json(os.path.join(OUTPUT_DIR, PDF_CONFIG["output"]), pdf_data)

          log("Done")

      - name: Commit and push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          if [ -n "$(git status --porcelain erd/)" ]; then
            git add erd/
            git commit -m "Updated $(TZ=America/Moncton date +'%Y/%m/%d %I:%M %p')"
            git push
          else
            echo "No changes to commit"
          fi
