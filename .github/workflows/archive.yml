name: Daily Fire Data Archive

on:
  schedule:
    - cron: "10 9 * * *"  # Daily at 09:10 UTC
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: fire-archive-${{ github.ref }}
  cancel-in-progress: false

jobs:
  archive:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install requests pdfplumber

      - name: Archive fire data
        shell: python
        run: |
          """Daily archive of fire data from CWFIS and ERD sources."""
          import os
          import json
          import time
          from datetime import datetime
          from zoneinfo import ZoneInfo
          import requests

          # =============================================================================
          # Configuration
          # =============================================================================

          ATLANTIC_TZ = ZoneInfo("America/Moncton")
          NB_BBOX = (-69.05, 44.56, -63.70, 48.07)  # minx, miny, maxx, maxy

          ARCHIVE_ROOT = "archive"
          CWFIS_DIR = os.path.join(ARCHIVE_ROOT, "cwfis")
          ERD_DIR = os.path.join(ARCHIVE_ROOT, "erd")

          TIMEOUT = 90
          RETRIES = 3

          CWFIS_WFS = "https://cwfis.cfs.nrcan.gc.ca/geoserver/public/ows"

          DATASETS = {
              # CWFIS WFS layers
              "24_hour_spots": {"type": "cwfis", "layer": "public:hotspots_last24hrs", "dir": CWFIS_DIR},
              "fire_perimeters": {"type": "cwfis", "layer": "public:m3_polygons_current", "dir": CWFIS_DIR},
              # ERD ArcGIS layers
              "active_fires": {"type": "erd", "url": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query", "dir": ERD_DIR},
              "out_fires": {"type": "erd", "url": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query", "dir": ERD_DIR},
              "fire_locations": {"type": "erd", "url": "https://gis-erd-der.gnb.ca/gisserver/rest/services/New_Brunswick_Fires/New_Brunswick_Fire_Locations/FeatureServer/0/query", "dir": ERD_DIR},
          }

          # ERD JSON endpoints (non-spatial / PDF-derived) archived with date suffix
          ERD_JSON_SOURCES = {
              "sums_table": {
                  "url": "https://gis-erd-der.gnb.ca/gisserver/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query",
                  "params": {"where": "1=1", "outFields": "*", "returnGeometry": "false", "f": "json"},
              },
              "GNBfireActSum": {
                  "url": "https://www3.gnb.ca/public/fire-feu/activitysum_e.pdf",
                  "is_pdf": True,
              },
          }

          # =============================================================================
          # Helpers
          # =============================================================================

          def log(msg: str) -> None:
              ts = time.strftime("%H:%M:%S", time.gmtime())
              print(f"[{ts}] {msg}", flush=True)

          def fetch_json(url: str, params: dict, label: str) -> dict:
              """Fetch JSON with retries."""
              for attempt in range(1, RETRIES + 1):
                  try:
                      log(f"  {label}: attempt {attempt}")
                      r = requests.get(url, params=params, timeout=TIMEOUT)
                      r.raise_for_status()
                      return r.json()
                  except Exception as e:
                      log(f"  {label}: error - {e}")
                      time.sleep(1 * attempt)
              return {}

          def empty_fc() -> dict:
              return {"type": "FeatureCollection", "features": []}

          def esri_to_geojson(esri: dict) -> dict:
              """Convert ESRI JSON to GeoJSON."""
              features = []
              for f in esri.get("features", []):
                  geom = f.get("geometry")
                  gj_geom = None
                  
                  if geom:
                      if "x" in geom and "y" in geom:
                          gj_geom = {"type": "Point", "coordinates": [geom["x"], geom["y"]]}
                      elif "rings" in geom:
                          rings = geom["rings"] or []
                          gj_geom = {"type": "Polygon", "coordinates": rings} if len(rings) == 1 else {"type": "MultiPolygon", "coordinates": [[r] for r in rings]}
                      elif "paths" in geom:
                          paths = geom["paths"] or []
                          gj_geom = {"type": "LineString", "coordinates": paths[0]} if len(paths) == 1 else {"type": "MultiLineString", "coordinates": paths}
                  
                  features.append({
                      "type": "Feature",
                      "geometry": gj_geom,
                      "properties": f.get("attributes", {}),
                  })
              
              return {"type": "FeatureCollection", "features": features}

          # =============================================================================
          # Fetchers
          # =============================================================================

          def fetch_cwfis(layer: str, label: str) -> dict:
              """Fetch from CWFIS WFS with NB bbox filter."""
              minx, miny, maxx, maxy = NB_BBOX
              
              params = {
                  "service": "WFS",
                  "version": "1.0.0",
                  "request": "GetFeature",
                  "typeName": layer,
                  "srsName": "EPSG:4326",
                  "bbox": f"{minx},{miny},{maxx},{maxy},EPSG:4326",
                  "outputFormat": "application/json",
              }
              
              data = fetch_json(CWFIS_WFS, params, label)
              
              if data.get("features"):
                  return data
              
              # Fallback: try CQL filter
              params.pop("bbox")
              params["CQL_FILTER"] = f"BBOX(the_geom,{minx},{miny},{maxx},{maxy})"
              data = fetch_json(CWFIS_WFS, params, f"{label} (CQL)")
              
              return data if data.get("features") else empty_fc()

          def fetch_erd(url: str, label: str) -> dict:
              """Fetch from ERD ArcGIS with NB bbox filter."""
              minx, miny, maxx, maxy = NB_BBOX
              
              # Try spatial filter first
              params = {
                  "where": "1=1",
                  "geometryType": "esriGeometryEnvelope",
                  "geometry": f"{minx},{miny},{maxx},{maxy}",
                  "inSR": 4326,
                  "spatialRel": "esriSpatialRelIntersects",
                  "outFields": "*",
                  "returnGeometry": "true",
                  "outSR": 4326,
                  "f": "json",
                  "resultRecordCount": 2000,
              }
              
              data = fetch_json(url, params, label)
              
              if data.get("error"):
                  log(f"  {label}: ArcGIS error - {data['error']}")
              elif data.get("features"):
                  return esri_to_geojson(data)
              
              # Fallback: all features
              params = {
                  "where": "1=1",
                  "outFields": "*",
                  "returnGeometry": "true",
                  "outSR": 4326,
                  "f": "json",
              }
              data = fetch_json(url, params, f"{label} (all)")
              
              return esri_to_geojson(data) if data.get("features") else empty_fc()

          def write_json(path: str, data: dict) -> None:
              os.makedirs(os.path.dirname(path), exist_ok=True)
              with open(path, "w", encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False)
                  f.write("\n")
              log(f"  Wrote {path}")

          # =============================================================================
          # Main
          # =============================================================================

          today = datetime.now(ATLANTIC_TZ).strftime("%Y%m%d")
          archived_at = datetime.now(ATLANTIC_TZ).strftime("%Y-%m-%d %H:%M:%S %Z")

          log(f"Archiving data for {today}")
          log(f"NB bbox: {NB_BBOX}")

          os.makedirs(CWFIS_DIR, exist_ok=True)
          os.makedirs(ERD_DIR, exist_ok=True)

          failures = []

          for name, config in DATASETS.items():
              log(f"Processing {name}...")
              
              try:
                  if config["type"] == "cwfis":
                      data = fetch_cwfis(config["layer"], name)
                  else:
                      data = fetch_erd(config["url"], name)
                  
                  data["archived_at"] = archived_at
                  
                  output = os.path.join(config["dir"], f"{name}_{today}.geojson")
                  write_json(output, data)
                  
                  log(f"  Features: {len(data.get('features', []))}")
                  
              except Exception as e:
                  log(f"  FAILED: {e}")
                  failures.append(name)

          # Archive ERD JSON sources (sums_table + GNBfireActSum)
          for name, cfg in ERD_JSON_SOURCES.items():
              log(f"Archiving {name}...")
              try:
                  if cfg.get("is_pdf"):
                      # PDF â†’ fetch raw bytes, save as-is with date suffix
                      import io
                      r = requests.get(cfg["url"], timeout=TIMEOUT)
                      r.raise_for_status()
                      # Try pdfplumber if available, otherwise save raw PDF
                      try:
                          import pdfplumber, re
                          def _clean(s):
                              return re.sub(r"\s+", " ", str(s or "")).strip()
                          result = {"source": cfg["url"], "archived_at": archived_at, "pages": 0, "tables": []}
                          with pdfplumber.open(io.BytesIO(r.content)) as pdf:
                              result["pages"] = len(pdf.pages)
                              for pn, page in enumerate(pdf.pages, 1):
                                  for tbl in (page.extract_tables() or []):
                                      if not tbl or len(tbl) < 2:
                                          continue
                                      hdr = [_clean(c) for c in tbl[0]]
                                      keep = [i for i, h in enumerate(hdr) if h]
                                      hdr = [hdr[i] for i in keep]
                                      if not hdr:
                                          continue
                                      rows = []
                                      for row in tbl[1:]:
                                          if not row:
                                              continue
                                          vals = [_clean(row[i]) if i < len(row) else "" for i in keep]
                                          vals = (vals + [""] * len(hdr))[:len(hdr)]
                                          if any(vals):
                                              rows.append(dict(zip(hdr, vals)))
                                      if rows:
                                          result["tables"].append({"page": pn, "headers": hdr, "rows": rows})
                          data = result
                      except ImportError:
                          data = {"source": cfg["url"], "archived_at": archived_at, "error": "pdfplumber not installed"}
                  else:
                      data = fetch_json(cfg["url"], cfg["params"], name)
                      data["archived_at"] = archived_at
                  
                  output = os.path.join(ERD_DIR, f"{name}_{today}.json")
                  write_json(output, data)
                  log(f"  Done")
              except Exception as e:
                  log(f"  FAILED: {e}")
                  failures.append(name)

          if failures:
              log(f"Failures: {failures}")
              raise SystemExit(1)

          log("Done")

      - name: Rebuild archive manifest
        shell: python
        run: |
          """Rebuild archive/manifest.json from the files on disk."""
          import os, json, re
          from datetime import datetime
          from zoneinfo import ZoneInfo

          ARCHIVE_ROOT = "archive"
          MANIFEST_PATH = os.path.join(ARCHIVE_ROOT, "manifest.json")
          ATLANTIC_TZ = ZoneInfo("America/Moncton")

          # Scan for all archived dates by looking at active_fires_*.geojson
          # (the most reliable indicator that a full archive exists for that date)
          date_re = re.compile(r"active_fires_(\d{8})\.geojson$")
          dates = set()
          erd_dir = os.path.join(ARCHIVE_ROOT, "erd")
          if os.path.isdir(erd_dir):
              for fn in os.listdir(erd_dir):
                  m = date_re.match(fn)
                  if m:
                      dates.add(m.group(1))

          manifest = {
              "generated": datetime.now(ATLANTIC_TZ).strftime("%Y-%m-%d"),
              "description": (
                  "Available archive dates for NBFireMap historical data. "
                  "Each date has active_fires, out_fires, fire_perimeters, "
                  "and 24h hotspots. fire_locations available from 2025-10-09 onward."
              ),
              "dates": sorted(dates),
              "files": {
                  "active_fires": "archive/erd/active_fires_{date}.geojson",
                  "out_fires": "archive/erd/out_fires_{date}.geojson",
                  "fire_locations": "archive/erd/fire_locations_{date}.geojson",
                  "fire_perimeters": "archive/cwfis/fire_perimeters_{date}.geojson",
                  "hotspots_24h": "archive/cwfis/24_hour_spots_{date}.geojson",
              },
          }

          with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
              json.dump(manifest, f, indent=2, ensure_ascii=False)
              f.write("\n")

          print(f"Manifest updated: {len(dates)} dates")

      - name: Commit and push
        env:
          BRANCH_NAME: ${{ github.ref_name }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git config pull.rebase true
          
          if [ -n "$(git status --porcelain archive/)" ]; then
            git add archive/ archive/manifest.json
            git commit -m "Archived $(TZ=America/Moncton date +'%Y/%m/%d %I:%M %p')"
            
            for i in 1 2 3; do
              if git push origin "HEAD:$BRANCH_NAME"; then
                exit 0
              fi
              echo "Push failed (attempt $i), rebasing..."
              git fetch origin "$BRANCH_NAME"
              git pull --rebase origin "$BRANCH_NAME" || true
              sleep 2
            done
            
            echo "Push failed after 3 attempts"
            exit 1
          else
            echo "No changes to commit"
          fi

      - name: Upload archive artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: geojson-archive
          path: |
            archive/**/*.geojson
            archive/**/*.json
          if-no-files-found: ignore
